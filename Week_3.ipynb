{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Support Vector Machines & Random Forests\n",
    "\n",
    "Here, we will explore the support vector machine (SVM) and random forest algorithms that we covered in the last class.\n"
   ],
   "metadata": {
    "id": "lhcT91UwAajV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction to the data\n",
    "\n",
    "-- We will use the Covertype dataset this week. Read more about the dataset here: https://scikit-learn.org/stable/datasets/real_world.html#covtype-dataset and https://archive.ics.uci.edu/ml/datasets/Covertype.\n",
    "\n",
    "\n",
    "-- Here are some prompts to help you find out more about the dataset:\n",
    "\n",
    "*   What type of labels does it have (real continuous or categorical)**?** What are the range of values for the labels**?** If categorical labels, have they been provided in numeric form, e.g. as integers**?**\n",
    "*   What is the feature dimensionality, i.e. the number of features**?** What are the range of values of each feature**?** Is the range the same across features**?**\n",
    "*   Can you find out how the data was collected**?**\n",
    "*   What of how it was labelled**?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "_p5-W8w2AwG_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data\n",
    "\n",
    "\n",
    "-- You need to download the data before you can get started. Download from https://archive.ics.uci.edu/ml/datasets/Covertype.\n",
    "\n",
    "-- You would have downloaded a *covtype.data.gz* file. Unzip this, e.g. using the free 7Zip software for those using Windows.\n",
    "\n",
    "-- The data file in the unzipped folder would be *covtype.data* which can be opened with any text editor. Use the file menu in Google Colab to upload the file to your Colab directory. Once upload is complete, you should be able to see the file on the listed contents of your Colab directory.\n",
    "\n",
    "-- You can now run the code in the cell below to load the data.\n"
   ],
   "metadata": {
    "id": "niqpu-upCGOx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "\n",
    "!ls  /content\n",
    "\n",
    "data_file_full_path = r\"C:\\Users\\Administrator\\Desktop\\master\\machine learnning\\covtype.data\\covtype.data\"\n",
    "\n",
    "covtype_data_as_list = []\n",
    "\n",
    "# load the dataset\n",
    "with open(data_file_full_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    for row in csv_reader:\n",
    "\n",
    "      covtype_data_as_list.append([float(val) for val in row])\n",
    "\n",
    "\n",
    "covtype_data = numpy.array(covtype_data_as_list)\n",
    "\n",
    "\n",
    "print(\"\\n The dataset has shape: \"+str(covtype_data.shape))\n",
    "\n",
    "\n",
    "# get the features and the labels\n",
    "feat_col = numpy.arange(0, 54)\n",
    "label_col = 54\n",
    "\n",
    "cov_type_feats = covtype_data[:, feat_col]\n",
    "cov_type_labels = covtype_data[:, label_col]\n",
    "\n",
    "print(\"\\n A peek at the dataset features: \\n\"+str(cov_type_feats))\n",
    "print(\"\\n A peek at the dataset labels: \\n\"+str(cov_type_labels))\n"
   ],
   "metadata": {
    "id": "FipQwcNeDI0A"
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The dataset has shape: (581012, 55)\n",
      "\n",
      " A peek at the dataset features: \n",
      "[[2.596e+03 5.100e+01 3.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.590e+03 5.600e+01 2.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.804e+03 1.390e+02 9.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [2.386e+03 1.590e+02 1.700e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.384e+03 1.700e+02 1.500e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.383e+03 1.650e+02 1.300e+01 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "\n",
      " A peek at the dataset labels: \n",
      "[5. 5. 2. ... 3. 3. 3.]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploring data & Splitting into training, validation, and test sets\n",
    "\n",
    "-- After you load the data, you can explore the relationships between each individual feature and the labels, e.g. using scatter plots or boxplots. The code below uses a histogram plot to check the distribution of the labels in the data.\n",
    "\n",
    "-- Next, you can split the data into training, validation, and test sets. The code below does that using a random split. What is the distribution of the labels in the training set**?**"
   ],
   "metadata": {
    "id": "xMSse3lcER0r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "random_seed = 1\n",
    "\n",
    "# Plot the frequencies of each class in the labels\n",
    "plt.figure()\n",
    "_, _, _ = plt.hist(cov_type_labels, bins=numpy.unique(cov_type_labels), align='left')\n",
    "plt.title('Class frequencies for full dataset')\n",
    "plt.show()\n",
    "print('\\n')\n",
    "\n",
    "# Let's only consider data with class labels '1' and '2' and only one tenth of that subset\n",
    "# You can use a larger portion of the dataset\n",
    "# But it will take longer to train your model\n",
    "all_ids = numpy.arange(0, cov_type_feats.shape[0])\n",
    "mid_sub_ids = all_ids[cov_type_labels<3]\n",
    "cov_type_labels_mid_sub = cov_type_labels[mid_sub_ids]\n",
    "cov_type_feats_mid_sub = cov_type_feats[mid_sub_ids, :]\n",
    "all_ids_sub = numpy.arange(0, cov_type_feats_mid_sub.shape[0])\n",
    "\n",
    "_, sub_ids = train_test_split(all_ids_sub, test_size=0.1, train_size=0.9, \n",
    "                                 random_state=random_seed, shuffle=True, stratify=cov_type_labels_mid_sub)\n",
    "cov_type_labels_sub = cov_type_labels_mid_sub[sub_ids]\n",
    "cov_type_feats_sub = cov_type_feats_mid_sub[sub_ids, :]\n",
    "\n",
    "# First randomly split the data 80/20 into training and test sets\n",
    "all_ids_sub = numpy.arange(0, cov_type_feats_sub.shape[0])\n",
    "train_set_ids, test_set_ids = train_test_split(all_ids_sub, test_size=0.2, train_size=0.8, \n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "\n",
    "\n",
    "# Then further split the training set into training and validation sets\n",
    "train_set_ids, val_set_ids = train_test_split(train_set_ids, test_size=0.1, train_size=0.9, \n",
    "                                 random_state=random_seed, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Show the distribution of the labels in the final training set\n",
    "plt.figure()\n",
    "_, _, _ = plt.hist(cov_type_labels_sub[train_set_ids], bins=numpy.unique(cov_type_labels), align='left')\n",
    "plt.title('Class frequencies for training set')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "id": "XkPQK1S7LB-M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing & Modelling\n",
    "\n",
    "-- Now that you have training, validation, and test sets, you can start modelling. Let's start with an SVM model: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html. Have a look at the parameters of the function for building the model. Which can you relate to the parameters that we considered in class**?** Which hyperparameters would you choose to optimize for your modelling**?**\n",
    "\n",
    "-- Build the model using the training set, use the validation set to select the optimal values for the hyperparameters that you have decided to optimize, then finally evaluate the model with these hyperparameter settings using the test set. \n",
    "\n",
    "-- The code below uses a linear SVM and optimizes C (the box constraint). The performance of the trained model is evaluated using F1 scores and confusion matrices, which are classification metrics that we will look at in the lectures on *Model Validation*. The code also computes accuracy just for the sake of completeness. \n",
    "\n",
    "-- Some things to try or think about from the results of code below are:\n",
    "\n",
    "\n",
    "*   Do you find any discrepancies between the accuracy, F1 scores, and average F1 scores: do they tell the same story about the performance of the model**?**\n",
    "*   What do you think that the confusion matrix shows**?**\n",
    "\n",
    "*   What do you notice about the range of values of the features in *scaled_cov_type_feats_sub*, compared to those in *cov_type_feats_sub* **?**\n",
    "*   What do you think would be the outcome if you used *cov_type_feats_sub* instead of *scaled_cov_type_feats_sub* **?** You can try using the former to see what happens.\n",
    "\n",
    "*   Can you see the use of the validation set**?**\n",
    "*   What if you optimized for a different range of the hyperparameter values**?**\n",
    "\n",
    "*   Do you think that the results reflect imbalance in the class distribution in the training set**?** You could try setting the *class_weight* parameter of the *SVC* function to 'balanced' to see what changes that leads to in performance.\n"
   ],
   "metadata": {
    "id": "vx8zGHSpE_X1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "# Scaling the features to the same range of values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cov_type_feats_sub)\n",
    "scaled_cov_type_feats_sub = scaler.transform(cov_type_feats_sub)\n",
    "print(\"\\n A peek at the scaled dataset features: \\n\"+str(scaled_cov_type_feats_sub))\n",
    "\n",
    "\n",
    "# Use the validation set to optimize the hyperparameters you wish to\n",
    "c_options = [0.1, 1.0, 10.0]\n",
    "best_c = 0.1\n",
    "best_c_perf = 0\n",
    "val_labels = cov_type_labels_sub[val_set_ids]\n",
    "train_labels = cov_type_labels_sub[train_set_ids];\n",
    "\n",
    "for c in c_options:\n",
    "  print(\"\\n for c=\"+str(c)+\"...\")\n",
    "  model = SVC(C=c, kernel='rbf', degree=3, gamma='scale', class_weight=None, random_state=random_seed)\n",
    "  model.fit(scaled_cov_type_feats_sub[train_set_ids, :], train_labels)\n",
    "  val_pred = model.predict(scaled_cov_type_feats_sub[val_set_ids, :])\n",
    "\n",
    "  avg_f1_score = f1_score(val_labels, val_pred, average='macro')\n",
    "\n",
    "  if avg_f1_score > best_c_perf:\n",
    "    best_c = c\n",
    "    best_c_perf = avg_f1_score\n",
    "\n",
    "print('\\n The optimal c for this data is: '+str(best_c))\n",
    "\n",
    "# Use the optimized hyperparameter to train the final model\n",
    "model = SVC(C=best_c, kernel='rbf', degree=3, gamma='scale', class_weight=None, random_state=random_seed)\n",
    "model.fit(scaled_cov_type_feats_sub[train_set_ids, :], train_labels)\n",
    "\n",
    "# Evaluate the trained model using the test set\n",
    "test_labels = cov_type_labels_sub[test_set_ids]\n",
    "test_pred = model.predict(scaled_cov_type_feats_sub[test_set_ids, :])\n",
    "\n",
    "avg_f1_score = f1_score(test_labels, test_pred, average='macro')\n",
    "f1_scores = f1_score(test_labels, test_pred, average=None)\n",
    "print('\\n The F1 scores for each of the classes are: '+str(f1_scores))\n",
    "print('\\n The average F1 score is: '+str(avg_f1_score))\n",
    "\n",
    "\n",
    "acc = accuracy_score(test_labels, test_pred)\n",
    "print('\\n The overall accuracy is: '+str(acc))\n",
    "\n",
    "confusion_matrix = confusion_matrix(test_labels, test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "sETTFPn0HBCL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing the SVM with a Random Forest\n",
    "\n",
    "-- You can build additional models using the Random Forest classifier. You can find the documentations in https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html for the *scikit learn* library. An hyperparameter that you could optimize here is the number of trees, i.e. 'n_estimators'. How does the performance of the Random Forest compare with the performance of the SVM**?** \n",
    "\n"
   ],
   "metadata": {
    "id": "UOMlgRBcJkeE"
   }
  }
 ]
}